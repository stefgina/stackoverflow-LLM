{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "seEZ7T55HDcc",
        "outputId": "5f92730d-341e-4f12-a2e3-b679163225df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.12.2)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from seaborn) (1.23.5)\n",
            "Requirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.10/dist-packages (from seaborn) (1.5.3)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in /usr/local/lib/python3.10/dist-packages (from seaborn) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.42.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->seaborn) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.16.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.42.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.31.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.6.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.12.2)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.65.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2023.7.22)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1--p20cXTZvk57GvPTxPmoylPtImvh0Vf\n",
            "To: /content/best_model.pt\n",
            "100% 1.31G/1.31G [00:05<00:00, 252MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Udrd9a944rJH0GxDhR6052gGNksb7rXO\n",
            "To: /content/df_eda.pkl\n",
            "100% 60.9M/60.9M [00:00<00:00, 153MB/s]\n"
          ]
        }
      ],
      "source": [
        "############################### deps\n",
        "!pip install scikit-learn\n",
        "!pip install seaborn\n",
        "!pip install matplotlib\n",
        "!pip install numpy\n",
        "!pip install pandas\n",
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install gdown\n",
        "!gdown --id 1--p20cXTZvk57GvPTxPmoylPtImvh0Vf # best_model.pt from google drive\n",
        "!gdown --id 1Udrd9a944rJH0GxDhR6052gGNksb7rXO # df_eda.pkl from google drive\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "############################### imports\n",
        "from sklearn.metrics import hamming_loss\n",
        "from sklearn.metrics import jaccard_score\n",
        "from sklearn.metrics import precision_recall_fscore_support as score\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn import metrics\n",
        "from sklearn.svm import LinearSVC\n",
        "import transformers\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertTokenizer, BertModel, BertConfig\n",
        "import seaborn as sns\n",
        "import shutil, sys\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "import torch\n",
        "import os"
      ],
      "metadata": {
        "id": "ycPmeG1aHN-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############################### CONFIG\n",
        "MAX_LEN = 225\n",
        "TRAIN_BATCH_SIZE = 36\n",
        "VALID_BATCH_SIZE = 36\n",
        "EPOCHS = 5\n",
        "LEARNING_RATE = 1e-05\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': False,\n",
        "                'num_workers': 0\n",
        "                }"
      ],
      "metadata": {
        "id": "x7Zz3uT3HUHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################ INIT\n",
        "warnings.simplefilter(\"ignore\")\n",
        "warnings.filterwarnings('ignore')\n",
        "sns.set_style(\"darkgrid\")"
      ],
      "metadata": {
        "id": "dB72ctnNHVi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################ READS\n",
        "df = pd.read_pickle(\"df_eda.pkl\")"
      ],
      "metadata": {
        "id": "gkim_v2rHXWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############################### CUDA\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZflle4OHY23",
        "outputId": "f2387d5d-fe12-4f58-ace9-264575e1c8f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "############################### JOIN TITLE + BODY\n",
        "df['Combo'] = df['Title'] + \". \" + df['Body']"
      ],
      "metadata": {
        "id": "ngFhp99WHZv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############################### BINARIZATION\n",
        "mlb = MultiLabelBinarizer()\n",
        "tag_df = pd.DataFrame(mlb.fit_transform(df['Tags']), columns=mlb.classes_, index=df.index)\n",
        "class_names = mlb.classes_"
      ],
      "metadata": {
        "id": "n4X4kI_NHbCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############################### DATAFRAME HOUSEKEEPING\n",
        "df = df.join(tag_df)\n",
        "df = df.drop(columns='Tags')\n",
        "df['target_list'] = df.iloc[:, 3:103].values.tolist()\n",
        "df = df.drop(df.columns[3:103], axis=1)\n",
        "df = df.drop(df.columns[0:2], axis=1)\n",
        "\n",
        "# DEBUG\n",
        "# print(df.head(2))\n",
        "# print(df.shape)"
      ],
      "metadata": {
        "id": "4tBg4emxHcU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############################### SPLIT\n",
        "# cross checking that my train and test split is exatcly the same with\n",
        "# the train test split i did for the model A.\n",
        "# reason for the check is the difference in data structures (df vs array)\n",
        "# (for example pd.sample(random_state=0) returns different split than sklearn for the same state)\n",
        "\n",
        "# so the split is 80/20 for train-val/test\n",
        "# and another 80/20 for train/val\n",
        "# so train: 72%, val 8%, and test 20%\n",
        "\n",
        "# Splitting the dataframe\n",
        "train_dataset, test_dataset = train_test_split(df, test_size=0.2, random_state=0)\n",
        "train_dataset, val_dataset = train_test_split(train_dataset, test_size=0.2, random_state=0)\n",
        "\n",
        "train_dataset = train_dataset.reset_index(drop=True)\n",
        "val_dataset = val_dataset.reset_index(drop=True)\n",
        "test_dataset = test_dataset.reset_index(drop=True)\n",
        "\n",
        "# DEBUG\n",
        "# print(Xy_train.head(1))\n",
        "# print(Xy_test.head(1))\n",
        "# quit()\n",
        "\n",
        "print(\"[PROGRAM]: full-set shape: {}\".format(df.shape))\n",
        "print(\"[PROGRAM]: train-set shape: {}\".format(train_dataset.shape))\n",
        "print(\"[PROGRAM]: val-set shape: {}\".format(val_dataset.shape))\n",
        "print(\"[PROGRAM]: test-set shape: {}\".format(test_dataset.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myXg6hziHfZ2",
        "outputId": "7287a822-9e21-49d8-9317-c9625d5c9c23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PROGRAM]: full-set shape: (80393, 2)\n",
            "[PROGRAM]: train-set shape: (51451, 2)\n",
            "[PROGRAM]: val-set shape: (12863, 2)\n",
            "[PROGRAM]: test-set shape: (16079, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "############################### TORCH DATASET\n",
        "class CustomDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.combo = dataframe['Combo']\n",
        "        self.targets = self.data.target_list\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.combo)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        combo = str(self.combo[index])\n",
        "        combo = \" \".join(combo.split())\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            combo,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "        token_type_ids = inputs[\"token_type_ids\"]\n",
        "\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
        "        }\n",
        "\n",
        "train_set = CustomDataset(train_dataset, tokenizer, MAX_LEN)\n",
        "val_set = CustomDataset(val_dataset, tokenizer, MAX_LEN)\n",
        "test_set = CustomDataset(test_dataset, tokenizer, MAX_LEN)\n",
        "\n",
        "# DEBUG\n",
        "# print(train_set[0])"
      ],
      "metadata": {
        "id": "XvrWA-NUHgW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############################### TORCH DATALOADER\n",
        "training_loader = DataLoader(train_set, **train_params)\n",
        "validation_loader = DataLoader(val_set, **test_params)\n",
        "test_loader = DataLoader(test_set, **test_params)"
      ],
      "metadata": {
        "id": "EsZPoy1pHjQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############################### TRAIN FUNCS\n",
        "# chckpoint and save funcs from here (joe)\n",
        "# https://towardsdatascience.com/how-to-save-and-load-a-model-in-pytorch-with-a-complete-example-c2920e617dee\n",
        "\n",
        "def loss_fn(outputs, targets):\n",
        "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)\n",
        "\n",
        "def load_ckp(checkpoint_fpath, model, optimizer):\n",
        "    # load check point\n",
        "    # initialize state_dict from checkpoint to model\n",
        "    # initialize optimizer from checkpoint to optimizer\n",
        "    # initialize valid_loss_min from checkpoint to valid_loss_min\n",
        "    # return model, optimizer, epoch value, min validation loss\n",
        "\n",
        "    checkpoint = torch.load(checkpoint_fpath)\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    valid_loss_min = checkpoint['valid_loss_min']\n",
        "    return model, optimizer, checkpoint['epoch'], valid_loss_min"
      ],
      "metadata": {
        "id": "oykYVYrmHlSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############################### MODEL\n",
        "# base : bert\n",
        "# extra dropout + linear layer\n",
        "# ending in 100 neurons, just like our classes\n",
        "# after i extract the propabillities of each of the 100 neurons\n",
        "# i select the proba >0.5 and bin the results to (0,1) (like sigmoid but manual)\n",
        "\n",
        "class BERTClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BERTClass, self).__init__()\n",
        "        self.l1 = transformers.BertModel.from_pretrained('bert-base-uncased', return_dict=False)\n",
        "        self.l2 = torch.nn.Dropout(0.3)\n",
        "        self.l3 = torch.nn.Linear(768, 100)\n",
        "\n",
        "    def forward(self, ids, mask, token_type_ids):\n",
        "        _, output_1= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids)\n",
        "        output_2 = self.l2(output_1)\n",
        "        output = self.l3(output_2)\n",
        "        return output\n",
        "\n",
        "model = BERTClass()\n",
        "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
        "model,_,_,_ = load_ckp(\"best_model.pt\", model, optimizer)\n",
        "model.to(device)\n",
        "print(\"[INFO]: BERT finetuned model loaded from best checkpoint\")\n",
        "print(\"[INFO]: model loaded to device\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b59mxUV-HnvV",
        "outputId": "163f97fa-aa29-4b2e-bac0-bcc81db41cb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO]: BERT finetuned model loaded from best checkpoint\n",
            "[INFO]: model loaded to device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "################################ METRICS\n",
        "def score_avg(y_pred, y_test):\n",
        "    precision = precision_score(y_test, y_pred, average='micro')\n",
        "    recall = recall_score(y_test, y_pred, average='micro')\n",
        "    f1 = f1_score(y_test, y_pred, average='micro')\n",
        "    hamming = hamming_loss(y_test, y_pred)\n",
        "    jacard = jaccard_score(y_test, y_pred, average='micro')\n",
        "\n",
        "    print(\"[PROGRAM]: classifier -> BERT finetuned\")\n",
        "    print(\"[PROGRAM]: avg precision: {}\".format(precision))\n",
        "    print(\"[PROGRAM]: avg recall: {}\".format(recall))\n",
        "    print(\"[PROGRAM]: avg f1-score: {}\".format(f1))\n",
        "    print(\"[PROGRAM]: avg hamming loss: {}\".format(hamming))\n",
        "    print(\"[PROGRAM]: avg jacard score: {}\".format(jacard))\n",
        "\n",
        "    return [precision, recall, f1, hamming, jacard]\n",
        "\n",
        "def score_per_tag(y_pred, y_test):\n",
        "    hamming = []\n",
        "    jaccard = []\n",
        "    precision, recall, fscore, support = score(y_test, y_pred)\n",
        "    for i, (test, pred) in enumerate(zip(y_test.T, y_pred.T)):\n",
        "        hamming.append(hamming_loss(test, pred))\n",
        "        jaccard.append(jaccard_score(test,pred))\n",
        "\n",
        "    # DEBUG\n",
        "    # print(len(precision))\n",
        "    # print(len(recall))\n",
        "    # print(len(fscore))\n",
        "    # print(len(support))\n",
        "    # print(len(hamming))\n",
        "    # print(len(jaccard))\n",
        "    # print(len(y_classes))\n",
        "\n",
        "    return pd.DataFrame(data=[precision, recall, fscore, support, hamming, jaccard],\n",
        "                         index=[\"Precision\", \"Recall\", \"F-1 score\", \"True count\", \"Hamming loss\", \"Jaccard score\"],\n",
        "                         columns=mlb.classes_)"
      ],
      "metadata": {
        "id": "AVcQSbQJHq0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################ INFERENCE TEST-SET\n",
        "model.eval()\n",
        "y_test = []\n",
        "y_pred = []\n",
        "with torch.no_grad():\n",
        "    for batch_idx, data in enumerate(test_loader, 0):\n",
        "        ids = data['ids'].to(device, dtype = torch.long)\n",
        "        mask = data['mask'].to(device, dtype = torch.long)\n",
        "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "        targets = data['targets'].to(device, dtype = torch.float)\n",
        "        outputs = model(ids, mask, token_type_ids)\n",
        "        y_test.extend(targets.cpu().detach().numpy().tolist())\n",
        "        y_pred.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
        "        print(\"[PROGRAM]: INFERENCE BATCH \", batch_idx,\" /446\")\n",
        "\n",
        "# applying hard map of probas into (0,1)\n",
        "y_pred = (np.array(y_pred) > 0.5).astype(int)\n",
        "y_pred = np.array(y_pred)\n",
        "y_test = np.array(y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4p4jynWHrkO",
        "outputId": "79e25b1c-cea2-438c-ea92-f76ae266fc6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PROGRAM]: INFERENCE BATCH  0  /446\n",
            "[PROGRAM]: INFERENCE BATCH  1  /446\n",
            "[PROGRAM]: INFERENCE BATCH  2  /446\n",
            "[PROGRAM]: INFERENCE BATCH  3  /446\n",
            "[PROGRAM]: INFERENCE BATCH  4  /446\n",
            "[PROGRAM]: INFERENCE BATCH  5  /446\n",
            "[PROGRAM]: INFERENCE BATCH  6  /446\n",
            "[PROGRAM]: INFERENCE BATCH  7  /446\n",
            "[PROGRAM]: INFERENCE BATCH  8  /446\n",
            "[PROGRAM]: INFERENCE BATCH  9  /446\n",
            "[PROGRAM]: INFERENCE BATCH  10  /446\n",
            "[PROGRAM]: INFERENCE BATCH  11  /446\n",
            "[PROGRAM]: INFERENCE BATCH  12  /446\n",
            "[PROGRAM]: INFERENCE BATCH  13  /446\n",
            "[PROGRAM]: INFERENCE BATCH  14  /446\n",
            "[PROGRAM]: INFERENCE BATCH  15  /446\n",
            "[PROGRAM]: INFERENCE BATCH  16  /446\n",
            "[PROGRAM]: INFERENCE BATCH  17  /446\n",
            "[PROGRAM]: INFERENCE BATCH  18  /446\n",
            "[PROGRAM]: INFERENCE BATCH  19  /446\n",
            "[PROGRAM]: INFERENCE BATCH  20  /446\n",
            "[PROGRAM]: INFERENCE BATCH  21  /446\n",
            "[PROGRAM]: INFERENCE BATCH  22  /446\n",
            "[PROGRAM]: INFERENCE BATCH  23  /446\n",
            "[PROGRAM]: INFERENCE BATCH  24  /446\n",
            "[PROGRAM]: INFERENCE BATCH  25  /446\n",
            "[PROGRAM]: INFERENCE BATCH  26  /446\n",
            "[PROGRAM]: INFERENCE BATCH  27  /446\n",
            "[PROGRAM]: INFERENCE BATCH  28  /446\n",
            "[PROGRAM]: INFERENCE BATCH  29  /446\n",
            "[PROGRAM]: INFERENCE BATCH  30  /446\n",
            "[PROGRAM]: INFERENCE BATCH  31  /446\n",
            "[PROGRAM]: INFERENCE BATCH  32  /446\n",
            "[PROGRAM]: INFERENCE BATCH  33  /446\n",
            "[PROGRAM]: INFERENCE BATCH  34  /446\n",
            "[PROGRAM]: INFERENCE BATCH  35  /446\n",
            "[PROGRAM]: INFERENCE BATCH  36  /446\n",
            "[PROGRAM]: INFERENCE BATCH  37  /446\n",
            "[PROGRAM]: INFERENCE BATCH  38  /446\n",
            "[PROGRAM]: INFERENCE BATCH  39  /446\n",
            "[PROGRAM]: INFERENCE BATCH  40  /446\n",
            "[PROGRAM]: INFERENCE BATCH  41  /446\n",
            "[PROGRAM]: INFERENCE BATCH  42  /446\n",
            "[PROGRAM]: INFERENCE BATCH  43  /446\n",
            "[PROGRAM]: INFERENCE BATCH  44  /446\n",
            "[PROGRAM]: INFERENCE BATCH  45  /446\n",
            "[PROGRAM]: INFERENCE BATCH  46  /446\n",
            "[PROGRAM]: INFERENCE BATCH  47  /446\n",
            "[PROGRAM]: INFERENCE BATCH  48  /446\n",
            "[PROGRAM]: INFERENCE BATCH  49  /446\n",
            "[PROGRAM]: INFERENCE BATCH  50  /446\n",
            "[PROGRAM]: INFERENCE BATCH  51  /446\n",
            "[PROGRAM]: INFERENCE BATCH  52  /446\n",
            "[PROGRAM]: INFERENCE BATCH  53  /446\n",
            "[PROGRAM]: INFERENCE BATCH  54  /446\n",
            "[PROGRAM]: INFERENCE BATCH  55  /446\n",
            "[PROGRAM]: INFERENCE BATCH  56  /446\n",
            "[PROGRAM]: INFERENCE BATCH  57  /446\n",
            "[PROGRAM]: INFERENCE BATCH  58  /446\n",
            "[PROGRAM]: INFERENCE BATCH  59  /446\n",
            "[PROGRAM]: INFERENCE BATCH  60  /446\n",
            "[PROGRAM]: INFERENCE BATCH  61  /446\n",
            "[PROGRAM]: INFERENCE BATCH  62  /446\n",
            "[PROGRAM]: INFERENCE BATCH  63  /446\n",
            "[PROGRAM]: INFERENCE BATCH  64  /446\n",
            "[PROGRAM]: INFERENCE BATCH  65  /446\n",
            "[PROGRAM]: INFERENCE BATCH  66  /446\n",
            "[PROGRAM]: INFERENCE BATCH  67  /446\n",
            "[PROGRAM]: INFERENCE BATCH  68  /446\n",
            "[PROGRAM]: INFERENCE BATCH  69  /446\n",
            "[PROGRAM]: INFERENCE BATCH  70  /446\n",
            "[PROGRAM]: INFERENCE BATCH  71  /446\n",
            "[PROGRAM]: INFERENCE BATCH  72  /446\n",
            "[PROGRAM]: INFERENCE BATCH  73  /446\n",
            "[PROGRAM]: INFERENCE BATCH  74  /446\n",
            "[PROGRAM]: INFERENCE BATCH  75  /446\n",
            "[PROGRAM]: INFERENCE BATCH  76  /446\n",
            "[PROGRAM]: INFERENCE BATCH  77  /446\n",
            "[PROGRAM]: INFERENCE BATCH  78  /446\n",
            "[PROGRAM]: INFERENCE BATCH  79  /446\n",
            "[PROGRAM]: INFERENCE BATCH  80  /446\n",
            "[PROGRAM]: INFERENCE BATCH  81  /446\n",
            "[PROGRAM]: INFERENCE BATCH  82  /446\n",
            "[PROGRAM]: INFERENCE BATCH  83  /446\n",
            "[PROGRAM]: INFERENCE BATCH  84  /446\n",
            "[PROGRAM]: INFERENCE BATCH  85  /446\n",
            "[PROGRAM]: INFERENCE BATCH  86  /446\n",
            "[PROGRAM]: INFERENCE BATCH  87  /446\n",
            "[PROGRAM]: INFERENCE BATCH  88  /446\n",
            "[PROGRAM]: INFERENCE BATCH  89  /446\n",
            "[PROGRAM]: INFERENCE BATCH  90  /446\n",
            "[PROGRAM]: INFERENCE BATCH  91  /446\n",
            "[PROGRAM]: INFERENCE BATCH  92  /446\n",
            "[PROGRAM]: INFERENCE BATCH  93  /446\n",
            "[PROGRAM]: INFERENCE BATCH  94  /446\n",
            "[PROGRAM]: INFERENCE BATCH  95  /446\n",
            "[PROGRAM]: INFERENCE BATCH  96  /446\n",
            "[PROGRAM]: INFERENCE BATCH  97  /446\n",
            "[PROGRAM]: INFERENCE BATCH  98  /446\n",
            "[PROGRAM]: INFERENCE BATCH  99  /446\n",
            "[PROGRAM]: INFERENCE BATCH  100  /446\n",
            "[PROGRAM]: INFERENCE BATCH  101  /446\n",
            "[PROGRAM]: INFERENCE BATCH  102  /446\n",
            "[PROGRAM]: INFERENCE BATCH  103  /446\n",
            "[PROGRAM]: INFERENCE BATCH  104  /446\n",
            "[PROGRAM]: INFERENCE BATCH  105  /446\n",
            "[PROGRAM]: INFERENCE BATCH  106  /446\n",
            "[PROGRAM]: INFERENCE BATCH  107  /446\n",
            "[PROGRAM]: INFERENCE BATCH  108  /446\n",
            "[PROGRAM]: INFERENCE BATCH  109  /446\n",
            "[PROGRAM]: INFERENCE BATCH  110  /446\n",
            "[PROGRAM]: INFERENCE BATCH  111  /446\n",
            "[PROGRAM]: INFERENCE BATCH  112  /446\n",
            "[PROGRAM]: INFERENCE BATCH  113  /446\n",
            "[PROGRAM]: INFERENCE BATCH  114  /446\n",
            "[PROGRAM]: INFERENCE BATCH  115  /446\n",
            "[PROGRAM]: INFERENCE BATCH  116  /446\n",
            "[PROGRAM]: INFERENCE BATCH  117  /446\n",
            "[PROGRAM]: INFERENCE BATCH  118  /446\n",
            "[PROGRAM]: INFERENCE BATCH  119  /446\n",
            "[PROGRAM]: INFERENCE BATCH  120  /446\n",
            "[PROGRAM]: INFERENCE BATCH  121  /446\n",
            "[PROGRAM]: INFERENCE BATCH  122  /446\n",
            "[PROGRAM]: INFERENCE BATCH  123  /446\n",
            "[PROGRAM]: INFERENCE BATCH  124  /446\n",
            "[PROGRAM]: INFERENCE BATCH  125  /446\n",
            "[PROGRAM]: INFERENCE BATCH  126  /446\n",
            "[PROGRAM]: INFERENCE BATCH  127  /446\n",
            "[PROGRAM]: INFERENCE BATCH  128  /446\n",
            "[PROGRAM]: INFERENCE BATCH  129  /446\n",
            "[PROGRAM]: INFERENCE BATCH  130  /446\n",
            "[PROGRAM]: INFERENCE BATCH  131  /446\n",
            "[PROGRAM]: INFERENCE BATCH  132  /446\n",
            "[PROGRAM]: INFERENCE BATCH  133  /446\n",
            "[PROGRAM]: INFERENCE BATCH  134  /446\n",
            "[PROGRAM]: INFERENCE BATCH  135  /446\n",
            "[PROGRAM]: INFERENCE BATCH  136  /446\n",
            "[PROGRAM]: INFERENCE BATCH  137  /446\n",
            "[PROGRAM]: INFERENCE BATCH  138  /446\n",
            "[PROGRAM]: INFERENCE BATCH  139  /446\n",
            "[PROGRAM]: INFERENCE BATCH  140  /446\n",
            "[PROGRAM]: INFERENCE BATCH  141  /446\n",
            "[PROGRAM]: INFERENCE BATCH  142  /446\n",
            "[PROGRAM]: INFERENCE BATCH  143  /446\n",
            "[PROGRAM]: INFERENCE BATCH  144  /446\n",
            "[PROGRAM]: INFERENCE BATCH  145  /446\n",
            "[PROGRAM]: INFERENCE BATCH  146  /446\n",
            "[PROGRAM]: INFERENCE BATCH  147  /446\n",
            "[PROGRAM]: INFERENCE BATCH  148  /446\n",
            "[PROGRAM]: INFERENCE BATCH  149  /446\n",
            "[PROGRAM]: INFERENCE BATCH  150  /446\n",
            "[PROGRAM]: INFERENCE BATCH  151  /446\n",
            "[PROGRAM]: INFERENCE BATCH  152  /446\n",
            "[PROGRAM]: INFERENCE BATCH  153  /446\n",
            "[PROGRAM]: INFERENCE BATCH  154  /446\n",
            "[PROGRAM]: INFERENCE BATCH  155  /446\n",
            "[PROGRAM]: INFERENCE BATCH  156  /446\n",
            "[PROGRAM]: INFERENCE BATCH  157  /446\n",
            "[PROGRAM]: INFERENCE BATCH  158  /446\n",
            "[PROGRAM]: INFERENCE BATCH  159  /446\n",
            "[PROGRAM]: INFERENCE BATCH  160  /446\n",
            "[PROGRAM]: INFERENCE BATCH  161  /446\n",
            "[PROGRAM]: INFERENCE BATCH  162  /446\n",
            "[PROGRAM]: INFERENCE BATCH  163  /446\n",
            "[PROGRAM]: INFERENCE BATCH  164  /446\n",
            "[PROGRAM]: INFERENCE BATCH  165  /446\n",
            "[PROGRAM]: INFERENCE BATCH  166  /446\n",
            "[PROGRAM]: INFERENCE BATCH  167  /446\n",
            "[PROGRAM]: INFERENCE BATCH  168  /446\n",
            "[PROGRAM]: INFERENCE BATCH  169  /446\n",
            "[PROGRAM]: INFERENCE BATCH  170  /446\n",
            "[PROGRAM]: INFERENCE BATCH  171  /446\n",
            "[PROGRAM]: INFERENCE BATCH  172  /446\n",
            "[PROGRAM]: INFERENCE BATCH  173  /446\n",
            "[PROGRAM]: INFERENCE BATCH  174  /446\n",
            "[PROGRAM]: INFERENCE BATCH  175  /446\n",
            "[PROGRAM]: INFERENCE BATCH  176  /446\n",
            "[PROGRAM]: INFERENCE BATCH  177  /446\n",
            "[PROGRAM]: INFERENCE BATCH  178  /446\n",
            "[PROGRAM]: INFERENCE BATCH  179  /446\n",
            "[PROGRAM]: INFERENCE BATCH  180  /446\n",
            "[PROGRAM]: INFERENCE BATCH  181  /446\n",
            "[PROGRAM]: INFERENCE BATCH  182  /446\n",
            "[PROGRAM]: INFERENCE BATCH  183  /446\n",
            "[PROGRAM]: INFERENCE BATCH  184  /446\n",
            "[PROGRAM]: INFERENCE BATCH  185  /446\n",
            "[PROGRAM]: INFERENCE BATCH  186  /446\n",
            "[PROGRAM]: INFERENCE BATCH  187  /446\n",
            "[PROGRAM]: INFERENCE BATCH  188  /446\n",
            "[PROGRAM]: INFERENCE BATCH  189  /446\n",
            "[PROGRAM]: INFERENCE BATCH  190  /446\n",
            "[PROGRAM]: INFERENCE BATCH  191  /446\n",
            "[PROGRAM]: INFERENCE BATCH  192  /446\n",
            "[PROGRAM]: INFERENCE BATCH  193  /446\n",
            "[PROGRAM]: INFERENCE BATCH  194  /446\n",
            "[PROGRAM]: INFERENCE BATCH  195  /446\n",
            "[PROGRAM]: INFERENCE BATCH  196  /446\n",
            "[PROGRAM]: INFERENCE BATCH  197  /446\n",
            "[PROGRAM]: INFERENCE BATCH  198  /446\n",
            "[PROGRAM]: INFERENCE BATCH  199  /446\n",
            "[PROGRAM]: INFERENCE BATCH  200  /446\n",
            "[PROGRAM]: INFERENCE BATCH  201  /446\n",
            "[PROGRAM]: INFERENCE BATCH  202  /446\n",
            "[PROGRAM]: INFERENCE BATCH  203  /446\n",
            "[PROGRAM]: INFERENCE BATCH  204  /446\n",
            "[PROGRAM]: INFERENCE BATCH  205  /446\n",
            "[PROGRAM]: INFERENCE BATCH  206  /446\n",
            "[PROGRAM]: INFERENCE BATCH  207  /446\n",
            "[PROGRAM]: INFERENCE BATCH  208  /446\n",
            "[PROGRAM]: INFERENCE BATCH  209  /446\n",
            "[PROGRAM]: INFERENCE BATCH  210  /446\n",
            "[PROGRAM]: INFERENCE BATCH  211  /446\n",
            "[PROGRAM]: INFERENCE BATCH  212  /446\n",
            "[PROGRAM]: INFERENCE BATCH  213  /446\n",
            "[PROGRAM]: INFERENCE BATCH  214  /446\n",
            "[PROGRAM]: INFERENCE BATCH  215  /446\n",
            "[PROGRAM]: INFERENCE BATCH  216  /446\n",
            "[PROGRAM]: INFERENCE BATCH  217  /446\n",
            "[PROGRAM]: INFERENCE BATCH  218  /446\n",
            "[PROGRAM]: INFERENCE BATCH  219  /446\n",
            "[PROGRAM]: INFERENCE BATCH  220  /446\n",
            "[PROGRAM]: INFERENCE BATCH  221  /446\n",
            "[PROGRAM]: INFERENCE BATCH  222  /446\n",
            "[PROGRAM]: INFERENCE BATCH  223  /446\n",
            "[PROGRAM]: INFERENCE BATCH  224  /446\n",
            "[PROGRAM]: INFERENCE BATCH  225  /446\n",
            "[PROGRAM]: INFERENCE BATCH  226  /446\n",
            "[PROGRAM]: INFERENCE BATCH  227  /446\n",
            "[PROGRAM]: INFERENCE BATCH  228  /446\n",
            "[PROGRAM]: INFERENCE BATCH  229  /446\n",
            "[PROGRAM]: INFERENCE BATCH  230  /446\n",
            "[PROGRAM]: INFERENCE BATCH  231  /446\n",
            "[PROGRAM]: INFERENCE BATCH  232  /446\n",
            "[PROGRAM]: INFERENCE BATCH  233  /446\n",
            "[PROGRAM]: INFERENCE BATCH  234  /446\n",
            "[PROGRAM]: INFERENCE BATCH  235  /446\n",
            "[PROGRAM]: INFERENCE BATCH  236  /446\n",
            "[PROGRAM]: INFERENCE BATCH  237  /446\n",
            "[PROGRAM]: INFERENCE BATCH  238  /446\n",
            "[PROGRAM]: INFERENCE BATCH  239  /446\n",
            "[PROGRAM]: INFERENCE BATCH  240  /446\n",
            "[PROGRAM]: INFERENCE BATCH  241  /446\n",
            "[PROGRAM]: INFERENCE BATCH  242  /446\n",
            "[PROGRAM]: INFERENCE BATCH  243  /446\n",
            "[PROGRAM]: INFERENCE BATCH  244  /446\n",
            "[PROGRAM]: INFERENCE BATCH  245  /446\n",
            "[PROGRAM]: INFERENCE BATCH  246  /446\n",
            "[PROGRAM]: INFERENCE BATCH  247  /446\n",
            "[PROGRAM]: INFERENCE BATCH  248  /446\n",
            "[PROGRAM]: INFERENCE BATCH  249  /446\n",
            "[PROGRAM]: INFERENCE BATCH  250  /446\n",
            "[PROGRAM]: INFERENCE BATCH  251  /446\n",
            "[PROGRAM]: INFERENCE BATCH  252  /446\n",
            "[PROGRAM]: INFERENCE BATCH  253  /446\n",
            "[PROGRAM]: INFERENCE BATCH  254  /446\n",
            "[PROGRAM]: INFERENCE BATCH  255  /446\n",
            "[PROGRAM]: INFERENCE BATCH  256  /446\n",
            "[PROGRAM]: INFERENCE BATCH  257  /446\n",
            "[PROGRAM]: INFERENCE BATCH  258  /446\n",
            "[PROGRAM]: INFERENCE BATCH  259  /446\n",
            "[PROGRAM]: INFERENCE BATCH  260  /446\n",
            "[PROGRAM]: INFERENCE BATCH  261  /446\n",
            "[PROGRAM]: INFERENCE BATCH  262  /446\n",
            "[PROGRAM]: INFERENCE BATCH  263  /446\n",
            "[PROGRAM]: INFERENCE BATCH  264  /446\n",
            "[PROGRAM]: INFERENCE BATCH  265  /446\n",
            "[PROGRAM]: INFERENCE BATCH  266  /446\n",
            "[PROGRAM]: INFERENCE BATCH  267  /446\n",
            "[PROGRAM]: INFERENCE BATCH  268  /446\n",
            "[PROGRAM]: INFERENCE BATCH  269  /446\n",
            "[PROGRAM]: INFERENCE BATCH  270  /446\n",
            "[PROGRAM]: INFERENCE BATCH  271  /446\n",
            "[PROGRAM]: INFERENCE BATCH  272  /446\n",
            "[PROGRAM]: INFERENCE BATCH  273  /446\n",
            "[PROGRAM]: INFERENCE BATCH  274  /446\n",
            "[PROGRAM]: INFERENCE BATCH  275  /446\n",
            "[PROGRAM]: INFERENCE BATCH  276  /446\n",
            "[PROGRAM]: INFERENCE BATCH  277  /446\n",
            "[PROGRAM]: INFERENCE BATCH  278  /446\n",
            "[PROGRAM]: INFERENCE BATCH  279  /446\n",
            "[PROGRAM]: INFERENCE BATCH  280  /446\n",
            "[PROGRAM]: INFERENCE BATCH  281  /446\n",
            "[PROGRAM]: INFERENCE BATCH  282  /446\n",
            "[PROGRAM]: INFERENCE BATCH  283  /446\n",
            "[PROGRAM]: INFERENCE BATCH  284  /446\n",
            "[PROGRAM]: INFERENCE BATCH  285  /446\n",
            "[PROGRAM]: INFERENCE BATCH  286  /446\n",
            "[PROGRAM]: INFERENCE BATCH  287  /446\n",
            "[PROGRAM]: INFERENCE BATCH  288  /446\n",
            "[PROGRAM]: INFERENCE BATCH  289  /446\n",
            "[PROGRAM]: INFERENCE BATCH  290  /446\n",
            "[PROGRAM]: INFERENCE BATCH  291  /446\n",
            "[PROGRAM]: INFERENCE BATCH  292  /446\n",
            "[PROGRAM]: INFERENCE BATCH  293  /446\n",
            "[PROGRAM]: INFERENCE BATCH  294  /446\n",
            "[PROGRAM]: INFERENCE BATCH  295  /446\n",
            "[PROGRAM]: INFERENCE BATCH  296  /446\n",
            "[PROGRAM]: INFERENCE BATCH  297  /446\n",
            "[PROGRAM]: INFERENCE BATCH  298  /446\n",
            "[PROGRAM]: INFERENCE BATCH  299  /446\n",
            "[PROGRAM]: INFERENCE BATCH  300  /446\n",
            "[PROGRAM]: INFERENCE BATCH  301  /446\n",
            "[PROGRAM]: INFERENCE BATCH  302  /446\n",
            "[PROGRAM]: INFERENCE BATCH  303  /446\n",
            "[PROGRAM]: INFERENCE BATCH  304  /446\n",
            "[PROGRAM]: INFERENCE BATCH  305  /446\n",
            "[PROGRAM]: INFERENCE BATCH  306  /446\n",
            "[PROGRAM]: INFERENCE BATCH  307  /446\n",
            "[PROGRAM]: INFERENCE BATCH  308  /446\n",
            "[PROGRAM]: INFERENCE BATCH  309  /446\n",
            "[PROGRAM]: INFERENCE BATCH  310  /446\n",
            "[PROGRAM]: INFERENCE BATCH  311  /446\n",
            "[PROGRAM]: INFERENCE BATCH  312  /446\n",
            "[PROGRAM]: INFERENCE BATCH  313  /446\n",
            "[PROGRAM]: INFERENCE BATCH  314  /446\n",
            "[PROGRAM]: INFERENCE BATCH  315  /446\n",
            "[PROGRAM]: INFERENCE BATCH  316  /446\n",
            "[PROGRAM]: INFERENCE BATCH  317  /446\n",
            "[PROGRAM]: INFERENCE BATCH  318  /446\n",
            "[PROGRAM]: INFERENCE BATCH  319  /446\n",
            "[PROGRAM]: INFERENCE BATCH  320  /446\n",
            "[PROGRAM]: INFERENCE BATCH  321  /446\n",
            "[PROGRAM]: INFERENCE BATCH  322  /446\n",
            "[PROGRAM]: INFERENCE BATCH  323  /446\n",
            "[PROGRAM]: INFERENCE BATCH  324  /446\n",
            "[PROGRAM]: INFERENCE BATCH  325  /446\n",
            "[PROGRAM]: INFERENCE BATCH  326  /446\n",
            "[PROGRAM]: INFERENCE BATCH  327  /446\n",
            "[PROGRAM]: INFERENCE BATCH  328  /446\n",
            "[PROGRAM]: INFERENCE BATCH  329  /446\n",
            "[PROGRAM]: INFERENCE BATCH  330  /446\n",
            "[PROGRAM]: INFERENCE BATCH  331  /446\n",
            "[PROGRAM]: INFERENCE BATCH  332  /446\n",
            "[PROGRAM]: INFERENCE BATCH  333  /446\n",
            "[PROGRAM]: INFERENCE BATCH  334  /446\n",
            "[PROGRAM]: INFERENCE BATCH  335  /446\n",
            "[PROGRAM]: INFERENCE BATCH  336  /446\n",
            "[PROGRAM]: INFERENCE BATCH  337  /446\n",
            "[PROGRAM]: INFERENCE BATCH  338  /446\n",
            "[PROGRAM]: INFERENCE BATCH  339  /446\n",
            "[PROGRAM]: INFERENCE BATCH  340  /446\n",
            "[PROGRAM]: INFERENCE BATCH  341  /446\n",
            "[PROGRAM]: INFERENCE BATCH  342  /446\n",
            "[PROGRAM]: INFERENCE BATCH  343  /446\n",
            "[PROGRAM]: INFERENCE BATCH  344  /446\n",
            "[PROGRAM]: INFERENCE BATCH  345  /446\n",
            "[PROGRAM]: INFERENCE BATCH  346  /446\n",
            "[PROGRAM]: INFERENCE BATCH  347  /446\n",
            "[PROGRAM]: INFERENCE BATCH  348  /446\n",
            "[PROGRAM]: INFERENCE BATCH  349  /446\n",
            "[PROGRAM]: INFERENCE BATCH  350  /446\n",
            "[PROGRAM]: INFERENCE BATCH  351  /446\n",
            "[PROGRAM]: INFERENCE BATCH  352  /446\n",
            "[PROGRAM]: INFERENCE BATCH  353  /446\n",
            "[PROGRAM]: INFERENCE BATCH  354  /446\n",
            "[PROGRAM]: INFERENCE BATCH  355  /446\n",
            "[PROGRAM]: INFERENCE BATCH  356  /446\n",
            "[PROGRAM]: INFERENCE BATCH  357  /446\n",
            "[PROGRAM]: INFERENCE BATCH  358  /446\n",
            "[PROGRAM]: INFERENCE BATCH  359  /446\n",
            "[PROGRAM]: INFERENCE BATCH  360  /446\n",
            "[PROGRAM]: INFERENCE BATCH  361  /446\n",
            "[PROGRAM]: INFERENCE BATCH  362  /446\n",
            "[PROGRAM]: INFERENCE BATCH  363  /446\n",
            "[PROGRAM]: INFERENCE BATCH  364  /446\n",
            "[PROGRAM]: INFERENCE BATCH  365  /446\n",
            "[PROGRAM]: INFERENCE BATCH  366  /446\n",
            "[PROGRAM]: INFERENCE BATCH  367  /446\n",
            "[PROGRAM]: INFERENCE BATCH  368  /446\n",
            "[PROGRAM]: INFERENCE BATCH  369  /446\n",
            "[PROGRAM]: INFERENCE BATCH  370  /446\n",
            "[PROGRAM]: INFERENCE BATCH  371  /446\n",
            "[PROGRAM]: INFERENCE BATCH  372  /446\n",
            "[PROGRAM]: INFERENCE BATCH  373  /446\n",
            "[PROGRAM]: INFERENCE BATCH  374  /446\n",
            "[PROGRAM]: INFERENCE BATCH  375  /446\n",
            "[PROGRAM]: INFERENCE BATCH  376  /446\n",
            "[PROGRAM]: INFERENCE BATCH  377  /446\n",
            "[PROGRAM]: INFERENCE BATCH  378  /446\n",
            "[PROGRAM]: INFERENCE BATCH  379  /446\n",
            "[PROGRAM]: INFERENCE BATCH  380  /446\n",
            "[PROGRAM]: INFERENCE BATCH  381  /446\n",
            "[PROGRAM]: INFERENCE BATCH  382  /446\n",
            "[PROGRAM]: INFERENCE BATCH  383  /446\n",
            "[PROGRAM]: INFERENCE BATCH  384  /446\n",
            "[PROGRAM]: INFERENCE BATCH  385  /446\n",
            "[PROGRAM]: INFERENCE BATCH  386  /446\n",
            "[PROGRAM]: INFERENCE BATCH  387  /446\n",
            "[PROGRAM]: INFERENCE BATCH  388  /446\n",
            "[PROGRAM]: INFERENCE BATCH  389  /446\n",
            "[PROGRAM]: INFERENCE BATCH  390  /446\n",
            "[PROGRAM]: INFERENCE BATCH  391  /446\n",
            "[PROGRAM]: INFERENCE BATCH  392  /446\n",
            "[PROGRAM]: INFERENCE BATCH  393  /446\n",
            "[PROGRAM]: INFERENCE BATCH  394  /446\n",
            "[PROGRAM]: INFERENCE BATCH  395  /446\n",
            "[PROGRAM]: INFERENCE BATCH  396  /446\n",
            "[PROGRAM]: INFERENCE BATCH  397  /446\n",
            "[PROGRAM]: INFERENCE BATCH  398  /446\n",
            "[PROGRAM]: INFERENCE BATCH  399  /446\n",
            "[PROGRAM]: INFERENCE BATCH  400  /446\n",
            "[PROGRAM]: INFERENCE BATCH  401  /446\n",
            "[PROGRAM]: INFERENCE BATCH  402  /446\n",
            "[PROGRAM]: INFERENCE BATCH  403  /446\n",
            "[PROGRAM]: INFERENCE BATCH  404  /446\n",
            "[PROGRAM]: INFERENCE BATCH  405  /446\n",
            "[PROGRAM]: INFERENCE BATCH  406  /446\n",
            "[PROGRAM]: INFERENCE BATCH  407  /446\n",
            "[PROGRAM]: INFERENCE BATCH  408  /446\n",
            "[PROGRAM]: INFERENCE BATCH  409  /446\n",
            "[PROGRAM]: INFERENCE BATCH  410  /446\n",
            "[PROGRAM]: INFERENCE BATCH  411  /446\n",
            "[PROGRAM]: INFERENCE BATCH  412  /446\n",
            "[PROGRAM]: INFERENCE BATCH  413  /446\n",
            "[PROGRAM]: INFERENCE BATCH  414  /446\n",
            "[PROGRAM]: INFERENCE BATCH  415  /446\n",
            "[PROGRAM]: INFERENCE BATCH  416  /446\n",
            "[PROGRAM]: INFERENCE BATCH  417  /446\n",
            "[PROGRAM]: INFERENCE BATCH  418  /446\n",
            "[PROGRAM]: INFERENCE BATCH  419  /446\n",
            "[PROGRAM]: INFERENCE BATCH  420  /446\n",
            "[PROGRAM]: INFERENCE BATCH  421  /446\n",
            "[PROGRAM]: INFERENCE BATCH  422  /446\n",
            "[PROGRAM]: INFERENCE BATCH  423  /446\n",
            "[PROGRAM]: INFERENCE BATCH  424  /446\n",
            "[PROGRAM]: INFERENCE BATCH  425  /446\n",
            "[PROGRAM]: INFERENCE BATCH  426  /446\n",
            "[PROGRAM]: INFERENCE BATCH  427  /446\n",
            "[PROGRAM]: INFERENCE BATCH  428  /446\n",
            "[PROGRAM]: INFERENCE BATCH  429  /446\n",
            "[PROGRAM]: INFERENCE BATCH  430  /446\n",
            "[PROGRAM]: INFERENCE BATCH  431  /446\n",
            "[PROGRAM]: INFERENCE BATCH  432  /446\n",
            "[PROGRAM]: INFERENCE BATCH  433  /446\n",
            "[PROGRAM]: INFERENCE BATCH  434  /446\n",
            "[PROGRAM]: INFERENCE BATCH  435  /446\n",
            "[PROGRAM]: INFERENCE BATCH  436  /446\n",
            "[PROGRAM]: INFERENCE BATCH  437  /446\n",
            "[PROGRAM]: INFERENCE BATCH  438  /446\n",
            "[PROGRAM]: INFERENCE BATCH  439  /446\n",
            "[PROGRAM]: INFERENCE BATCH  440  /446\n",
            "[PROGRAM]: INFERENCE BATCH  441  /446\n",
            "[PROGRAM]: INFERENCE BATCH  442  /446\n",
            "[PROGRAM]: INFERENCE BATCH  443  /446\n",
            "[PROGRAM]: INFERENCE BATCH  444  /446\n",
            "[PROGRAM]: INFERENCE BATCH  445  /446\n",
            "[PROGRAM]: INFERENCE BATCH  446  /446\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "################################# METRICS (micro-average)\n",
        "print(\"[INFO]: computing micro-average metrics for all tags\")\n",
        "metrics_avg = score_avg(y_pred, y_test)\n",
        "metrics_per_tag = score_per_tag(y_pred, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmhjtIvZHt_8",
        "outputId": "55676c99-b9ac-4ebf-8856-4def8e254eb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO]: computing micro-average metrics for all tags\n",
            "[PROGRAM]: classifier -> BERT finetuned\n",
            "[PROGRAM]: avg precision: 0.8367903538362161\n",
            "[PROGRAM]: avg recall: 0.4596310021288339\n",
            "[PROGRAM]: avg f1-score: 0.5933484312577928\n",
            "[PROGRAM]: avg hamming loss: 0.009939050936003483\n",
            "[PROGRAM]: avg jacard score: 0.42181620839363243\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "################################ METRICS ON TOP TEN TAGS\n",
        "top_ten_tags = [\"javascript\", \"java\", \"c#\", \"php\", \"android\", \"jquery\", \"python\", \"html\", \"c++\", \"ios\"]\n",
        "print(\"[INFO]: computing top-ten tag metrics\")\n",
        "print(metrics_per_tag[top_ten_tags])\n",
        "print(\"[INFO]: computing top-ten tag metrics averaged\")\n",
        "print(metrics_per_tag[top_ten_tags].apply(np.mean, axis=1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1ZegTaKHvWI",
        "outputId": "e4cbfe3b-6136-4c09-9698-b9450d8b0b1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO]: computing top-ten tag metrics\n",
            "                javascript         java           c#         php      android  \\\n",
            "Precision         0.792972     0.878280     0.753503    0.890080     0.947538   \n",
            "Recall            0.598566     0.693725     0.662745    0.794258     0.905166   \n",
            "F-1 score         0.682190     0.775169     0.705216    0.839444     0.925868   \n",
            "True count     1395.000000  1737.000000  1785.000000  836.000000  1297.000000   \n",
            "Hamming loss      0.048386     0.043473     0.061509    0.015797     0.011692   \n",
            "Jaccard score     0.517669     0.632878     0.544659    0.723312     0.861968   \n",
            "\n",
            "                   jquery       python        html          c++         ios  \n",
            "Precision        0.849408     0.924547    0.723214     0.840095    0.623580  \n",
            "Recall           0.699164     0.838504    0.155172     0.636528    0.664145  \n",
            "F-1 score        0.766998     0.879426    0.255521     0.724280    0.643223  \n",
            "True count     718.000000  1096.000000  522.000000  1106.000000  661.000000  \n",
            "Hamming loss     0.018969     0.015673    0.029355     0.033335    0.030288  \n",
            "Jaccard score    0.622057     0.784799    0.146474     0.567742    0.474082  \n",
            "[INFO]: computing top-ten tag metrics averaged\n",
            "Precision           0.822322\n",
            "Recall              0.664797\n",
            "F-1 score           0.719733\n",
            "True count       1115.300000\n",
            "Hamming loss        0.030848\n",
            "Jaccard score       0.587564\n",
            "dtype: float64\n"
          ]
        }
      ]
    }
  ]
}