{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYyl0X7hF9Wf"
      },
      "outputs": [],
      "source": [
        "############################### deps\n",
        "!pip install scikit-learn\n",
        "!pip install seaborn\n",
        "!pip install matplotlib\n",
        "!pip install numpy\n",
        "!pip install pandas\n",
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install gdown\n",
        "!gdown --id 1Udrd9a944rJH0GxDhR6052gGNksb7rXO # df_eda.pkl from google drive\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "############################### imports\n",
        "from sklearn.metrics import hamming_loss\n",
        "from sklearn.metrics import jaccard_score\n",
        "from sklearn.metrics import precision_recall_fscore_support as score\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn import metrics\n",
        "from sklearn.svm import LinearSVC\n",
        "import transformers\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertTokenizer, BertModel, BertConfig\n",
        "import seaborn as sns\n",
        "import shutil, sys\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "import torch\n",
        "import os"
      ],
      "metadata": {
        "id": "CzLS29EkGIKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############################### CONFIG\n",
        "MAX_LEN = 225\n",
        "TRAIN_BATCH_SIZE = 36\n",
        "VALID_BATCH_SIZE = 36\n",
        "EPOCHS = 5\n",
        "LEARNING_RATE = 1e-05\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': False,\n",
        "                'num_workers': 0\n",
        "                }"
      ],
      "metadata": {
        "id": "h3mHxpMbGSYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################ INIT\n",
        "warnings.simplefilter(\"ignore\")\n",
        "sns.set_style(\"darkgrid\")\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_colwidth', None)"
      ],
      "metadata": {
        "id": "LX43Oi8wGU-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############################### CUDA\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "KddLjDEuGWyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############################### JOIN TITLE + BODY\n",
        "df = pd.read_pickle(\"df_eda.pkl\")\n",
        "df['Combo'] = df['Title'] + \". \" + df['Body']"
      ],
      "metadata": {
        "id": "C7pl4kgfGZ-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############################### BINARIZATION\n",
        "mlb = MultiLabelBinarizer()\n",
        "tag_df = pd.DataFrame(mlb.fit_transform(df['Tags']), columns=mlb.classes_, index=df.index)\n",
        "class_names = mlb.classes_"
      ],
      "metadata": {
        "id": "kfa6Z8tlGfC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############################### DATAFRAME HOUSEKEEPING\n",
        "df = df.join(tag_df)\n",
        "df = df.drop(columns='Tags')\n",
        "df['target_list'] = df.iloc[:, 3:103].values.tolist()\n",
        "df = df.drop(df.columns[3:103], axis=1)\n",
        "df = df.drop(df.columns[0:2], axis=1)\n",
        "\n",
        "# DEBUG\n",
        "# print(df.head(2))\n",
        "# print(df.shape)"
      ],
      "metadata": {
        "id": "tUZczNVwGgza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############################### SPLIT\n",
        "# cross checking that my train and test split is exatcly the same with\n",
        "# the train test split i did for the model A.\n",
        "# reason for the check is the difference in data structures (df vs array)\n",
        "# (for example pd.sample(random_state=0) returns different split than sklearn for the same state)\n",
        "\n",
        "# so the split is 80/20 for train-val/test\n",
        "# and another 80/20 for train/val\n",
        "# so train: 72%, val 8%, and test 20%\n",
        "\n",
        "# Splitting the dataframe\n",
        "train_dataset, test_dataset = train_test_split(df, test_size=0.2, random_state=0)\n",
        "train_dataset, val_dataset = train_test_split(train_dataset, test_size=0.2, random_state=0)\n",
        "\n",
        "train_dataset = train_dataset.reset_index(drop=True)\n",
        "val_dataset = val_dataset.reset_index(drop=True)\n",
        "test_dataset = test_dataset.reset_index(drop=True)\n",
        "\n",
        "# Xy_train, Xy_test = train_test_split(df, test_size = 0.2, random_state = 0)\n",
        "# Xy_train, Xy_val = train_test_split(Xy_train, test_size = 0.1, random_state = 0)\n",
        "\n",
        "# # Resetting the indices\n",
        "# train_dataset = test_dataset.reset_index(drop=True)\n",
        "# val_dataset = valid_dataset.reset_index(drop=True)\n",
        "\n",
        "\n",
        "# DEBUG\n",
        "# print(Xy_train.head(1))\n",
        "# print(Xy_test.head(1))\n",
        "# quit()\n",
        "\n",
        "print(\"[PROGRAM]: full-set shape: {}\".format(df.shape))\n",
        "print(\"[PROGRAM]: train-set shape: {}\".format(train_dataset.shape))\n",
        "print(\"[PROGRAM]: val-set shape: {}\".format(val_dataset.shape))\n",
        "print(\"[PROGRAM]: test-set shape: {}\".format(test_dataset.shape))"
      ],
      "metadata": {
        "id": "oDJns_iUGieW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############################### TORCH DATASET\n",
        "class CustomDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.combo = dataframe['Combo']\n",
        "        self.targets = self.data.target_list\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.combo)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        combo = str(self.combo[index])\n",
        "        combo = \" \".join(combo.split())\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            combo,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "        token_type_ids = inputs[\"token_type_ids\"]\n",
        "\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
        "        }\n",
        "\n",
        "\n",
        "train_set = CustomDataset(train_dataset, tokenizer, MAX_LEN)\n",
        "val_set = CustomDataset(val_dataset, tokenizer, MAX_LEN)\n",
        "test_set = CustomDataset(test_dataset, tokenizer, MAX_LEN)\n",
        "\n",
        "# DEBUG\n",
        "# print(train_set[0])"
      ],
      "metadata": {
        "id": "fdoHpJ8mGlU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############################### TORCH DATALOADER\n",
        "training_loader = DataLoader(train_set, **train_params)\n",
        "validation_loader = DataLoader(val_set, **test_params)\n",
        "test_loader = DataLoader(test_set, **test_params)\n",
        "\n",
        "len(training_loader)"
      ],
      "metadata": {
        "id": "GdN64eHSGnrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############################### TRAIN FUNCS\n",
        "# chckpoint and save funcs from here\n",
        "# https://towardsdatascience.com/how-to-save-and-load-a-model-in-pytorch-with-a-complete-example-c2920e617dee\n",
        "\n",
        "def loss_fn(outputs, targets):\n",
        "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)\n",
        "\n",
        "def load_ckp(checkpoint_fpath, model, optimizer):\n",
        "    # load check point\n",
        "    # initialize state_dict from checkpoint to model\n",
        "    # initialize optimizer from checkpoint to optimizer\n",
        "    # initialize valid_loss_min from checkpoint to valid_loss_min\n",
        "    # return model, optimizer, epoch value, min validation loss\n",
        "\n",
        "    checkpoint = torch.load(checkpoint_fpath)\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    valid_loss_min = checkpoint['valid_loss_min']\n",
        "    return model, optimizer, checkpoint['epoch'], valid_loss_min\n",
        "\n",
        "def save_ckp(state, is_best, checkpoint_path, best_model_path):\n",
        "    # save checkpoint data to the path given, checkpoint_path\n",
        "    # if it is a best model, min validation loss\n",
        "    # copy that checkpoint file to best path given, best_model_path\n",
        "\n",
        "    f_path = checkpoint_path\n",
        "    torch.save(state, f_path)\n",
        "    if is_best:\n",
        "        best_fpath = best_model_path\n",
        "        shutil.copyfile(f_path, best_fpath)"
      ],
      "metadata": {
        "id": "6IwIMhxcGrbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############################### MODEL\n",
        "# base : bert\n",
        "# extra dropout + linear layer\n",
        "# ending in 100 neurons, just like our classes\n",
        "# after i extract the propabillities of each of the 100 neurons\n",
        "# i select the proba >0.5 and bin the results to (0,1) (like sigmoid but manual)\n",
        "\n",
        "class BERTClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BERTClass, self).__init__()\n",
        "        self.l1 = transformers.BertModel.from_pretrained('bert-base-uncased', return_dict=False)\n",
        "        self.l2 = torch.nn.Dropout(0.3)\n",
        "        self.l3 = torch.nn.Linear(768, 100)\n",
        "\n",
        "    def forward(self, ids, mask, token_type_ids):\n",
        "        _, output_1= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids)\n",
        "        output_2 = self.l2(output_1)\n",
        "        output = self.l3(output_2)\n",
        "        return output\n",
        "\n",
        "model = BERTClass()\n",
        "model.to(device)\n",
        "print(\"[INFO]: model loaded to device\")"
      ],
      "metadata": {
        "id": "rjpTXcEBGs2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############################### LOOP\n",
        "# globals\n",
        "val_targets=[]\n",
        "val_outputs=[]\n",
        "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "print(\"[INFO]: starting training\")\n",
        "def train_model(start_epochs,  n_epochs, valid_loss_min_input,\n",
        "                training_loader, validation_loader, model,\n",
        "                optimizer, checkpoint_path, best_model_path):\n",
        "\n",
        "    # initialize tracker for minimum validation loss\n",
        "    valid_loss_min = valid_loss_min_input\n",
        "\n",
        "    for epoch in range(start_epochs, n_epochs+1):\n",
        "        train_loss = 0\n",
        "        valid_loss = 0\n",
        "\n",
        "        model.train()\n",
        "        print('[PROGRAM]: epoch', epoch)\n",
        "        print('[PROGRAM]: TRAINING START')\n",
        "        for batch_idx, data in enumerate(training_loader):\n",
        "            print('[PROGRAM]: TRAINING batch ', batch_idx, ' /1420')\n",
        "            ids = data['ids'].to(device, dtype = torch.long)\n",
        "            mask = data['mask'].to(device, dtype = torch.long)\n",
        "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "            targets = data['targets'].to(device, dtype = torch.float)\n",
        "            outputs = model(ids, mask, token_type_ids)\n",
        "            optimizer.zero_grad()\n",
        "            loss = loss_fn(outputs, targets)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.item() - train_loss))\n",
        "\n",
        "        model.eval()\n",
        "        print('[PROGRAM]: epoch', epoch)\n",
        "        print('[PROGRAM]: VALIDATION START')\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, data in enumerate(validation_loader, 0):\n",
        "                print('[PROGRAM]: VALIDATION batch ', batch_idx)\n",
        "                ids = data['ids'].to(device, dtype = torch.long)\n",
        "                mask = data['mask'].to(device, dtype = torch.long)\n",
        "                token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "                targets = data['targets'].to(device, dtype = torch.float)\n",
        "                outputs = model(ids, mask, token_type_ids)\n",
        "                loss = loss_fn(outputs, targets)\n",
        "                valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.item() - valid_loss))\n",
        "                val_targets.extend(targets.cpu().detach().numpy().tolist())\n",
        "                val_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
        "\n",
        "            print('[PROGRAM]: VALIDATION END')\n",
        "            # calculate average losses\n",
        "            train_loss = train_loss/len(training_loader)\n",
        "            valid_loss = valid_loss/len(validation_loader)\n",
        "            print('[PROGRAM]: Epoch: {} \\tAvgerage Training Loss: {:.6f} \\tAverage Validation Loss: {:.6f}'.format(\n",
        "                epoch,\n",
        "                train_loss,\n",
        "                valid_loss\n",
        "                ))\n",
        "\n",
        "            # create checkpoint variable and add important data\n",
        "            checkpoint = {\n",
        "                'epoch': epoch + 1,\n",
        "                'valid_loss_min': valid_loss,\n",
        "                'state_dict': model.state_dict(),\n",
        "                'optimizer': optimizer.state_dict()\n",
        "            }\n",
        "\n",
        "            # save checkpoint\n",
        "            save_ckp(checkpoint, False, checkpoint_path, best_model_path)\n",
        "            if valid_loss <= valid_loss_min:\n",
        "                print('[PROGRAM]: Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,valid_loss))\n",
        "                # save checkpoint as best model\n",
        "                save_ckp(checkpoint, True, checkpoint_path, best_model_path)\n",
        "                valid_loss_min = valid_loss\n",
        "\n",
        "        print('[PROGRAM]: Epoch {}  Done \\n'.format(epoch))\n",
        "\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "yEPc9ThCGvN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############################### DIRS\n",
        "# check if the directory already exists\n",
        "# if it does not exist, create the directory\n",
        "\n",
        "directory_checkpoints = \"/checkpoints\"\n",
        "directory_best_models = \"/best_models\"\n",
        "\n",
        "if not os.path.exists(directory_checkpoints):\n",
        "    os.makedirs(directory_checkpoints)\n",
        "    print(\"Directory '/checkpoints' created successfully.\")\n",
        "else:\n",
        "    print(\"Directory '/checkpoints' already exists.\")\n",
        "\n",
        "if not os.path.exists(directory_best_models):\n",
        "    os.makedirs(directory_best_models)\n",
        "    print(\"Directory '/best_models' created successfully.\")\n",
        "else:\n",
        "    print(\"Directory '/best_models' already exists.\")\n",
        "\n",
        "checkpoint_path = \"/checkpoints/current_checkpoint.pt\"\n",
        "best_model = \"/best_models/best_model.pt\""
      ],
      "metadata": {
        "id": "U9zdOpSMGx9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############################### RUN\n",
        "trained_model = train_model(1, EPOCHS, np.Inf, training_loader, validation_loader, model,\n",
        "                      optimizer,checkpoint_path,best_model)"
      ],
      "metadata": {
        "id": "WbogwtC-G0N_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}